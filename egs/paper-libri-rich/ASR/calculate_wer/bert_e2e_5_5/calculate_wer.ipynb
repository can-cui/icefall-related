{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# model_silero_te, example_texts, languages, punct, apply_te = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "#                                                                   model='silero_te')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "# load model and processor\n",
    "import os\n",
    "# import librosa\n",
    "import jiwer\n",
    "import string\n",
    "import csv\n",
    "from autocorrect import Speller\n",
    "spell = Speller()\n",
    "from transformers import pipeline\n",
    "\n",
    "# fix_spelling = pipeline(\"text2text-generation\",model=\"oliverguhr/spelling-correction-english-base\")\n",
    "\n",
    "\n",
    "def get_gold_hyp_lists(gold_path,test_path,septype):\n",
    "    def create_hyp_sentence_dictionary(filename):\n",
    "        sentence_dict = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for idx,line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    sentence_id, sentence = line.split(septype,1)\n",
    "                    sentence_id=sentence_id+\".wav\" # for AMI\n",
    "                    sentence_dict[sentence_id] = sentence\n",
    "        return sentence_dict\n",
    "\n",
    "    def create_gold_sentence_dictionary(filename):\n",
    "        sentence_dict = {}\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                # parts = line.split(' ', 1) # Libri\n",
    "                parts = line.split(',', 1) # AMI\n",
    "                sentence_id, sentence = parts\n",
    "                if sentence.strip()!='':\n",
    "                    sentence_dict[sentence_id] = sentence.strip()\n",
    "        return sentence_dict\n",
    "    \n",
    "    def read_csv_to_dict(file_path):\n",
    "        result_dict = {}\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            for row in csv_reader:\n",
    "                key, value = row[0], row[1]\n",
    "                key=key.split('/')[1]\n",
    "                # print(key,value)\n",
    "                # break\n",
    "                result_dict[key] = value\n",
    "        return result_dict\n",
    "    \n",
    "    gold_dict = create_gold_sentence_dictionary(gold_path)\n",
    "    # gold_dict = read_csv_to_dict(gold_path)\n",
    "    test_dict = create_hyp_sentence_dictionary(test_path)\n",
    "    \n",
    "    gold_list = []\n",
    "    test_list = []\n",
    "    for test_key, test_value in test_dict.items():\n",
    "        if test_key in gold_dict:\n",
    "            gold = gold_dict[test_key]\n",
    "            gold_list.append(gold)\n",
    "            test_list.append(test_value)\n",
    "    print(len(test_list))\n",
    "    return gold_list,test_list\n",
    "\n",
    "def add_space_before_punctuation(text):\n",
    "    # List of punctuation marks\n",
    "    punctuation_marks = string.punctuation\n",
    "\n",
    "    # Iterate over each character in the text\n",
    "    modified_text = ''\n",
    "    for char in text:\n",
    "        if char in punctuation_marks:\n",
    "            modified_text += ' ' + char\n",
    "        else:\n",
    "            modified_text += char\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "def add_space_before_punctuation(text):\n",
    "    # List of punctuation marks\n",
    "    punctuation_marks = string.punctuation\n",
    "\n",
    "    # Iterate over each character in the text\n",
    "    modified_text = ''\n",
    "    for char in text:\n",
    "        if char in punctuation_marks:\n",
    "            modified_text += ' ' + char\n",
    "        else:\n",
    "            modified_text += char\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "def add_space(reference_list, hypothesis_list):\n",
    "    reference = []\n",
    "    hypothesis = []\n",
    "    for ref, hyp in zip(reference_list, hypothesis_list):\n",
    "        # Preprocess sentences to keep punctuation as separate words\n",
    "        reference.append(add_space_before_punctuation(ref))\n",
    "        hypothesis.append(add_space_before_punctuation(hyp))\n",
    "        # print(reference,hypothesis)\n",
    "        # Calculate WER\n",
    "       \n",
    "    #     wer = jiwer.wer(reference, hypothesis)\n",
    "    #     # print(wer)\n",
    "    #     wer_scores.append(wer)\n",
    "    # wer_mean=sum(wer_scores) / len(wer_scores)\n",
    "    \n",
    "    return reference,hypothesis\n",
    "\n",
    "def get_detail_wer(gold_list,test_list):\n",
    "    out = jiwer.process_words(\n",
    "            gold_list,\n",
    "                test_list,\n",
    "            )\n",
    "    print(\"wer: \",out.wer*100)\n",
    "    print(\"hits: \",out.hits)\n",
    "    print(\"substitutions: \",out.substitutions)\n",
    "    print(\"insertions: \",out.insertions)\n",
    "    print(\"deletions: \",out.deletions)\n",
    "    print(\"total words: \",out.hits+out.substitutions+out.deletions)\n",
    "\n",
    "# def get_t_p_wer(substitutions, insertions,deletions,total_words_punc, P_WER):\n",
    "#     t_p_wer=(substitutions+insertions+deletions)/total_words_punc\n",
    "#     print(\"Topline of P-WER is \",t_p_wer*100)\n",
    "#     print(\"Relative PER is \", P_WER-t_p_wer*100)\n",
    "\n",
    "def get_c_p_er_fr_wer(wer,C_WER,P_WER,clean=True):\n",
    "    if clean:\n",
    "        total_words=50082\n",
    "        total_words_punt=57312\n",
    "        cased_words=3849\n",
    "        p_num = 9709\n",
    "    else:\n",
    "        total_words=48488\n",
    "        total_words_punt=55896\n",
    "        cased_words=4231\n",
    "        p_num = 9408\n",
    "    error_words = wer*total_words\n",
    "    error_c = C_WER*total_words\n",
    "    cer=(error_c-error_words)/cased_words\n",
    "    error_p = P_WER*total_words_punt\n",
    "    per=(error_p-error_words)/p_num\n",
    "    # print(\"Topline of P-WER is \",t_p_wer)\n",
    "    print(\"Relative CER is \", cer)\n",
    "    print(\"Relative PER is \", per)\n",
    "\n",
    "def get_c_p_er_fr_wer_ami(wer,C_WER,P_WER):\n",
    "    # if clean:\n",
    "    total_words=94006\n",
    "    total_words_punt=110266\n",
    "    cased_words=7626\n",
    "    p_num = 16260\n",
    "    error_words = wer*total_words\n",
    "    error_c = C_WER*total_words\n",
    "    cer=(error_c-error_words)/cased_words\n",
    "    error_p = P_WER*total_words_punt\n",
    "    per=(error_p-error_words)/p_num\n",
    "    print(\"Relative CER is \", cer)\n",
    "    print(\"Relative PER is \", per)\n",
    "\n",
    "def get_cased_word(text_list):\n",
    "    capitalized_word_count = 0\n",
    "\n",
    "    # Iterate through the list of text\n",
    "    for text in text_list:\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "        \n",
    "        # Check each word in the text\n",
    "        for word in words:\n",
    "            # Check if the word is capitalized (first character is uppercase)\n",
    "            if word.isalpha() and word[0].isupper():\n",
    "                capitalized_word_count += 1\n",
    "    print(\"Cased words number is \", capitalized_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/9kglh8tx5nb_wn200vv8zgwh00mc7h/T/ipykernel_92668/358533432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgold_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gold_hyp_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/ccui/Desktop/POC/dictation/books-transcription/test-clean-rich-book.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"clean.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# get_cased_word(gold_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgold_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0madd_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CP-WER\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_detail_wer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l5/9kglh8tx5nb_wn200vv8zgwh00mc7h/T/ipykernel_92668/1126786627.py\u001b[0m in \u001b[0;36mget_gold_hyp_lists\u001b[0;34m(gold_path, test_path, septype)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mgold_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gold_sentence_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# gold_dict = read_csv_to_dict(gold_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hyp_sentence_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l5/9kglh8tx5nb_wn200vv8zgwh00mc7h/T/ipykernel_92668/1126786627.py\u001b[0m in \u001b[0;36mcreate_gold_sentence_dictionary\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# parts = line.split(' ', 1) # Libri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# AMI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0msentence_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0msentence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "gold_list,test_list = get_gold_hyp_lists(\"/Users/ccui/Desktop/POC/dictation/books-transcription/test-clean-rich-book.txt\",\"clean.txt\",',')\n",
    "# get_cased_word(gold_list)\n",
    "gold_list,test_list= add_space(gold_list,test_list)\n",
    "print(\"CP-WER\")\n",
    "total_wer = get_detail_wer(gold_list, test_list)\n",
    "gold_rc= jiwer.ToLowerCase()(gold_list)\n",
    "test_rc = jiwer.ToLowerCase()(test_list)\n",
    "print(\"P-WER\")\n",
    "wer_rc = get_detail_wer(gold_rc, test_rc)\n",
    "# print(\"WER removing case:\", wer_rc*100)\n",
    "gold_rp= jiwer.RemovePunctuation()(gold_list)\n",
    "test_rp = jiwer.RemovePunctuation()(test_list)\n",
    "# gold_rp,test_rp= add_space(gold_rp,test_rp)\n",
    "print(\"C-WER\")\n",
    "wer_rp = get_detail_wer(gold_rp, test_rp)\n",
    "# print(\"WER removing punctuation:\", wer_rp*100)\n",
    "gold_rc_rp= jiwer.RemovePunctuation()(gold_rc)\n",
    "test_rc_rp = jiwer.RemovePunctuation()(test_rc)\n",
    "wer_rc_rp = get_detail_wer(gold_rc_rp, test_rc_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative CER is  80.90103503048464\n",
      "Relative PER is  44.60098858856002\n"
     ]
    }
   ],
   "source": [
    "get_c_p_er_fr_wer(wer=2.2674650698602794,C_WER=7.9128070417991605,P_WER=9.537098405832491,clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2731\n",
      "CP-WER\n",
      "wer:  18.42241733874573\n",
      "hits:  46071\n",
      "substitutions:  6713\n",
      "insertions:  452\n",
      "deletions:  3137\n",
      "total words:  55921\n",
      "P-WER\n",
      "wer:  12.598129504121886\n",
      "hits:  49324\n",
      "substitutions:  3464\n",
      "insertions:  448\n",
      "deletions:  3133\n",
      "total words:  55921\n",
      "C-WER\n",
      "wer:  12.035376337435833\n",
      "hits:  42902\n",
      "substitutions:  5335\n",
      "insertions:  233\n",
      "deletions:  270\n",
      "total words:  48507\n",
      "WER\n",
      "wer:  5.2404807553548975\n",
      "hits:  46197\n",
      "substitutions:  2041\n",
      "insertions:  232\n",
      "deletions:  269\n",
      "total words:  48507\n"
     ]
    }
   ],
   "source": [
    "gold_list,test_list = get_gold_hyp_lists(\"/Users/ccui/Desktop/dictation/books-transcription/test-other-rich-book.txt\",\"other.txt\",',')\n",
    "# get_cased_word(gold_list)\n",
    "gold_list,test_list= add_space(gold_list,test_list)\n",
    "print(\"CP-WER\")\n",
    "total_wer = get_detail_wer(gold_list, test_list)\n",
    "# print(\"Total WER:\", total_wer*100)\n",
    "gold_rc= jiwer.ToLowerCase()(gold_list)\n",
    "test_rc = jiwer.ToLowerCase()(test_list)\n",
    "print(\"P-WER\")\n",
    "wer_rc = get_detail_wer(gold_rc, test_rc)\n",
    "# print(\"WER removing case:\", wer_rc*100)\n",
    "gold_rp= jiwer.RemovePunctuation()(gold_list)\n",
    "test_rp = jiwer.RemovePunctuation()(test_list)\n",
    "# gold_rp,test_rp= add_space(gold_rp,test_rp)\n",
    "print(\"C-WER\")\n",
    "wer_rp = get_detail_wer(gold_rp, test_rp)\n",
    "# print(\"WER removing punctuation:\", wer_rp*100)\n",
    "gold_rc_rp= jiwer.RemovePunctuation()(gold_rc)\n",
    "test_rc_rp = jiwer.RemovePunctuation()(test_rc)\n",
    "print(\"WER\")\n",
    "wer_rc_rp = get_detail_wer(gold_rc_rp, test_rc_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative CER is  73.99740951653988\n",
      "Relative PER is  47.84062668970543\n"
     ]
    }
   ],
   "source": [
    "get_c_p_er_fr_wer(wer=5.2404807553548975,C_WER=11.69739874876523,P_WER=12.598129504121886,clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9474\n",
      "CP-WER\n",
      "wer:  45.245134492953405\n",
      "hits:  63499\n",
      "substitutions:  26188\n",
      "insertions:  3123\n",
      "deletions:  20579\n",
      "total words:  110266\n",
      "P-WER\n",
      "wer:  42.38296483050079\n",
      "hits:  66730\n",
      "substitutions:  22882\n",
      "insertions:  3198\n",
      "deletions:  20654\n",
      "total words:  110266\n",
      "C-WER\n",
      "wer:  42.23772950662724\n",
      "hits:  56542\n",
      "substitutions:  22490\n",
      "insertions:  2242\n",
      "deletions:  14974\n",
      "total words:  94006\n",
      "wer:  38.777312086462565\n",
      "hits:  59844\n",
      "substitutions:  19139\n",
      "insertions:  2291\n",
      "deletions:  15023\n",
      "total words:  94006\n"
     ]
    }
   ],
   "source": [
    "gold_list,test_list = get_gold_hyp_lists(\"/Users/ccui/Desktop/dictation/vasr/calculate_wer/ami_test_gold_process.txt\",\"ihm.txt\",',')\n",
    "# get_cased_word(gold_list)\n",
    "gold_list,test_list= add_space(gold_list,test_list)\n",
    "print(\"CP-WER\")\n",
    "total_wer = get_detail_wer(gold_list, test_list)\n",
    "# print(\"Total WER:\", total_wer*100)\n",
    "gold_rc= jiwer.ToLowerCase()(gold_list)\n",
    "test_rc = jiwer.ToLowerCase()(test_list)\n",
    "print(\"P-WER\")\n",
    "wer_rc = get_detail_wer(gold_rc, test_rc)\n",
    "# print(\"WER removing case:\", wer_rc*100)\n",
    "gold_rp= jiwer.RemovePunctuation()(gold_list)\n",
    "test_rp = jiwer.RemovePunctuation()(test_list)\n",
    "print(\"C-WER\")\n",
    "wer_rp = get_detail_wer(gold_rp, test_rp)\n",
    "# print(\"WER removing punctuation:\", wer_rp*100)\n",
    "gold_rc_rp= jiwer.RemovePunctuation()(gold_rc)\n",
    "test_rc_rp = jiwer.RemovePunctuation()(test_rc)\n",
    "wer_rc_rp = get_detail_wer(gold_rc_rp, test_rc_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative CER is  36.62470495672692\n",
      "Relative PER is  95.6519065190652\n"
     ]
    }
   ],
   "source": [
    "get_c_p_er_fr_wer_ami(wer=38.777312086462565,C_WER=42.23772950662724,P_WER=42.38296483050079)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed and saved to ihm_processed.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output file paths\n",
    "input_file_path = 'ihm.txt'\n",
    "output_file_path = 'ihm_processed.txt'\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "    # Iterate through each line in the input file\n",
    "    for line in input_file:\n",
    "        # Split the line into ID and text\n",
    "        parts = line.split(',')\n",
    "        if len(parts) == 2:\n",
    "            id = parts[0]\n",
    "            text = parts[1].strip()\n",
    "            \n",
    "            # Capitalize the first word of the text\n",
    "            text = text.capitalize()\n",
    "            \n",
    "            # Write the processed line to the output file\n",
    "            output_file.write(f\"{id},{text}\\n\")\n",
    "\n",
    "print(\"File processed and saved to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
